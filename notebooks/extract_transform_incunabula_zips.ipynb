{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5354fc4d",
   "metadata": {},
   "source": [
    "# Extracting zipped parsed page xml files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574f8a6f",
   "metadata": {},
   "source": [
    "This nb originally extracted the zip files containing the results of Isaac's parsing that were uploaded to the project [github](https://github.com/Southampton-Digital-Humanities/2023_Catalogue-Heading-Detection). Once we took over maintaining the code we've switched to processing the data locally so the zipping is obsolete. The majority of the code is now about parsing the xmls exported from Transkribus using updated versions of the code that Isaac wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cabb5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if '../' not in sys.path:\n",
    "    sys.path.append('../')\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "from xml.etree import ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "from src.data import xml_extraction as xmle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68affd28",
   "metadata": {},
   "source": [
    "### Extract zips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e1b9da",
   "metadata": {},
   "source": [
    "For when we were extracting zips of Isaac's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1e7834",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_data_path = \"\\\\\\\\ad\\\\collections\\\\TwoCenturies\\\\TwoCenturies IV\\\\Incunabula\\\\split_data\"\n",
    "\n",
    "zips = glob.glob(\"\\\\\\\\ad\\\\collections\\\\TwoCenturies\\\\TwoCenturies IV\\\\Incunabula\\\\split_data\\\\*.zip\")\n",
    "\n",
    "# [shutil.unpack_archive(x, x[:-4], format=\"zip\") for x in zips]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e6aa17",
   "metadata": {},
   "source": [
    "### Rename download jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e9c9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"..\\\\export_job_rename_dict.txt\") as f:  # .txt was a result of massaging the pairings out of Rossitza's .xlsx\n",
    "    rename_lines = f.readlines()\n",
    "    rename_split = [x.split(\":\") for x in rename_lines]\n",
    "    rename_dict = {x[0]:x[1].strip(\"\\n\") for x in rename_split}\n",
    "    \n",
    "rename_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb6bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for folder in done_zips:\n",
    "#     renamed = os.path.join(os.path.dirname(folder), export_dict[os.path.basename(folder)])\n",
    "#     os.rename(folder, renamed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76565f3d",
   "metadata": {},
   "source": [
    "#### All lines in entry honoured as separate lines in output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948dc109",
   "metadata": {},
   "source": [
    "This is a bit outdated now - I think Rossitza prefers to have all entries on one line for using in AntConc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dedef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_zip = done_zips[10:11]\n",
    "target_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0f128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in target_zip:\n",
    "    \n",
    "    txts = glob.glob(os.path.join(f, \"rawtextfiles\\\\*.txt\"))\n",
    "    raw_fname = os.path.join(f, \"combinedrawtext.txt\")\n",
    "    \n",
    "    with open(raw_fname, \"w\") as raw_out:\n",
    "        for t in tqdm(txts):\n",
    "            with open(t) as infile:\n",
    "                entry = infile.read()                \n",
    "                raw_out.write(entry)\n",
    "                raw_out.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d57268",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in target_zip:\n",
    "    print(f\"Combining  {f}\")\n",
    "    txts = glob.glob(os.path.join(f, \"splittextfiles\\\\*.txt\"))\n",
    "    split_fname = os.path.join(f, \"combinedsplittext.txt\")\n",
    "    \n",
    "    with open(split_fname, \"w\") as split_out:\n",
    "        for t in tqdm(txts):\n",
    "            with open(t) as infile:\n",
    "                entry = infile.readlines()\n",
    "                dash_line = \"-----------------------------------\\n\"\n",
    "                non_english_line = \"NON-ENGLISH SECTION LASTING\"\n",
    "                english_only_entry = [x for x in entry if dash_line not in x and non_english_line not in x]\n",
    "                split_str = \"\".join(english_only_entry)\n",
    "                split_out.write(split_str)\n",
    "                split_out.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb6f99b",
   "metadata": {},
   "source": [
    "#### All lines in entry concatenated onto one single line in output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbda4559",
   "metadata": {},
   "source": [
    "Better - but we're now combining all the entries from one volume together as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b30f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in target_zip:\n",
    "    \n",
    "    txts = glob.glob(os.path.join(f, \"rawtextfiles\\\\*.txt\"))\n",
    "    raw_fname = os.path.join(f, \"combinedrawtext_single_line.txt\")\n",
    "\n",
    "    with open(raw_fname, \"w\") as raw_out:\n",
    "        for t in tqdm(txts):\n",
    "            with open(t) as infile:\n",
    "                entry = infile.readlines()\n",
    "                raw_str = \" \".join([x.strip(\"\\n\") for x in entry])\n",
    "                raw_out.write(raw_str + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b598df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in target_zip:\n",
    "    print(f\"Combining  {f}\")\n",
    "    txts = glob.glob(os.path.join(f, \"splittextfiles\\\\*.txt\"))\n",
    "    split_fname = os.path.join(f, \"combinedsplittext_single_line.txt\")\n",
    "    \n",
    "    with open(split_fname, \"w\") as split_out:\n",
    "        for t in tqdm(txts):\n",
    "            with open(t) as infile:\n",
    "                entry = infile.readlines()\n",
    "                dash_line = \"-----------------------------------\\n\"\n",
    "                non_english_line = \"NON-ENGLISH SECTION LASTING\"\n",
    "                english_only_entry = [x.strip(\"\\n\") for x in entry if dash_line not in x and non_english_line not in x]\n",
    "                split_str = \" \".join(english_only_entry)\n",
    "                split_out.write(split_str + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c95fe",
   "metadata": {},
   "source": [
    "### Combined volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0e5936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paired globs\n",
    "combined_volumes = []\n",
    "for i in range(1):\n",
    "    network_loc = \"\\\\\\\\ad\\\\collections\\\\TwoCenturies\\\\TwoCenturies IV\\\\Incunabula\"\n",
    "    suffix = \"column pages Transkribus export\"\n",
    "    combined_volumes.append([os.path.join(network_loc, f\"BMC_{i+1} 2 {suffix}\"), os.path.join(network_loc, f\"BMC_{i+1} 4 {suffix}\")])    \n",
    "    \n",
    "#     When running locally\n",
    "#     combined_volumes.append([f'C:\\\\Users\\\\HLloyd\\\\Documents\\\\BMC_{i+1}_2', f'C:\\\\Users\\\\HLloyd\\\\Documents\\\\BMC_{i+1}_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a929a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079f41df",
   "metadata": {},
   "source": [
    "#### Raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d45d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(combined_volumes):\n",
    "    print(f\"Combining  {os.path.basename(v[0])}, {os.path.basename(v[1])}\")\n",
    "    txts = glob.glob(os.path.join(v[0], \"rawtextfiles\\\\*.txt\")) + glob.glob(os.path.join(v[1], \"rawtextfiles\\\\*.txt\"))\n",
    "    single_vol_dir = os.path.join(os.path.dirname(v[0]), \"split_data\\\\test\", f\"BMC_{i+1}\")\n",
    "    if not os.path.exists(single_vol_dir):\n",
    "        os.mkdir(single_vol_dir)\n",
    "    raw_fname = os.path.join(single_vol_dir, \"combinedrawtext_single_line.txt\")\n",
    "    \n",
    "#     with open(raw_fname, \"w\", encoding=\"utf-8-sig\") as raw_out:\n",
    "#         for t in tqdm(txts):\n",
    "#             with open(t, encoding=\"utf-8-sig\") as infile:\n",
    "#                 entry = infile.readlines()\n",
    "#                 raw_str = \" \".join([x.strip(\"\\n\") for x in entry])\n",
    "#                 raw_out.write(raw_str + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71999c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "txts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d0c37",
   "metadata": {},
   "source": [
    "#### Split text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4052078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(combined_volumes):\n",
    "    print(f\"Combining  {v[0]}\")\n",
    "    txts = glob.glob(os.path.join(v[0], \"splittextfiles\\\\*.txt\")) + glob.glob(os.path.join(v[1], \"splittextfiles\\\\*.txt\"))\n",
    "    single_vol_dir = os.path.join(os.path.dirname(v[0]), f\"BMC_{i+1}\")\n",
    "    if not os.path.exists(single_vol_dir):\n",
    "        os.mkdir(single_vol_dir)\n",
    "    split_fname = os.path.join(single_vol_dir, \"combinedsplittext_single_line.txt\")\n",
    "    \n",
    "    with open(split_fname, \"w\", encoding=\"utf-8-sig\") as split_out:\n",
    "        for t in tqdm(txts):\n",
    "            with open(t, encoding=\"utf-8-sig\") as infile:\n",
    "                entry = infile.readlines()\n",
    "                dash_line = \"-----------------------------------\\n\"\n",
    "                non_english_line = \"NON-ENGLISH SECTION LASTING\"\n",
    "                english_only_entry = [x.strip(\"\\n\") for x in entry if dash_line not in x and non_english_line not in x]\n",
    "                split_str = \" \".join(english_only_entry)\n",
    "                split_out.write(split_str + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66239890",
   "metadata": {},
   "source": [
    "### Refactor output check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e23c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "refactor_out = glob.glob(r\"C:\\Users\\HLloyd\\Documents\\BMC_*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e070607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "refactor_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b8286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in refactor_out:\n",
    "    shutil.copytree(x, os.path.join(ic_data_path, os.path.basename(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "refactor_sizes = {x: [os.path.getsize(y) for y in sorted(glob.glob(os.path.join(x, \"*.txt\")))] for x in refactor_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4715a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_out = glob.glob(r\"C:\\Users\\HLloyd\\Documents\\Incunabula-Catalogue-Entry-Detection\\data\\*\\rawtextfiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7deefe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acad605",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_paired_sizes = {x: [os.path.getsize(y) for y in sorted(glob.glob(os.path.join(x, \"*.txt\"))) if os.path.exists(os.path.join('C:\\\\Users\\\\HLloyd\\\\Documents\\\\', y[68:]))] for x in id_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a56e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor_sizes.pop(r\"C:\\Users\\HLloyd\\Documents\\BMC_5_2\\rawtextfiles\")\n",
    "for (me_k, me_v), (u_k, u_v) in zip(refactor_sizes.items(), id_paired_sizes.items()):\n",
    "    check = [x == y for x, y in zip(me_v, u_v)]\n",
    "    print(f\"{me_k[-25:-10]}: {sum(check) == len(check)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a53d11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "zips = glob.glob(r\"C:\\Users\\HLloyd\\Documents\\BMC*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437703ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf007522",
   "metadata": {},
   "outputs": [],
   "source": [
    "[shutil.unpack_archive(x, os.path.join(os.path.dirname(x), os.path.basename(x[:-4]))) for x in zips[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3ad586",
   "metadata": {},
   "source": [
    "## Running main.py to debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61250fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_xmls = []\n",
    "for i in range(8, 9):\n",
    "    network_loc = \"\\\\\\\\ad\\\\collections\\\\TwoCenturies\\\\TwoCenturies IV\\\\Incunabula\"\n",
    "    suffix = \"column pages Transkribus export\"\n",
    "    page_xmls.append(\n",
    "        [\n",
    "            os.path.join(network_loc, f\"BMC_{i + 1} 2 {suffix}\"),\n",
    "            os.path.join(network_loc, f\"BMC_{i + 1} 4 {suffix}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# this one didn't work for some reason\n",
    "# 'BMC_5 2 column pages Transkribus export'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429d95e",
   "metadata": {},
   "source": [
    "Use 8 as the test bed as it's got one of the lowest numbers of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ae04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in page_xmls:\n",
    "    pageXMLLocation_2 = os.path.join(x[0], r\"*\\*\\page\\*.xml\")\n",
    "    pageXMLLocation_4 = os.path.join(x[1], r\"*\\*\\page\\*.xml\")\n",
    "    out_path = os.path.join(network_loc, \"split_data\\\\test\", os.path.basename(x[0]).split(\" \")[0])\n",
    "    attempts = 0\n",
    "    while attempts < 3:\n",
    "        xmls_2 = glob.glob(os.fsencode(pageXMLLocation_2))\n",
    "        xmls_4 = glob.glob(os.fsencode(pageXMLLocation_4))\n",
    "\n",
    "        if xmls_2 and xmls_4:\n",
    "            xmls = xmls_2 + xmls_4\n",
    "            break\n",
    "        else:\n",
    "            attempts += 1\n",
    "            continue\n",
    "    else:\n",
    "        raise IOError(f\"Failed to connect to {os.path.dirname(pageXMLLocation_2)}  {os.path.basename(pageXMLLocation_2)}/{os.path.basename(pageXMLLocation_4)}\")\n",
    "    \n",
    "    xmlroots = {}\n",
    "\n",
    "    print(f\"\\nGetting xml roots from {os.path.dirname(pageXMLLocation_2)}  {os.path.basename(pageXMLLocation_2)}/{os.path.basename(pageXMLLocation_4)}\")\n",
    "    for file in tqdm(xmls):\n",
    "        fileName = os.fsdecode(file)\n",
    "        attempts = 0\n",
    "        while attempts < 3:\n",
    "            try:\n",
    "                tree = ET.parse(fileName)\n",
    "                break\n",
    "            except FileNotFoundError:\n",
    "                attempts += 1\n",
    "                continue\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Failed to connect to: {fileName}\")\n",
    "        root = tree.getroot()\n",
    "        xmlroots[fileName.split(\"_\")[-1][:-4]] = root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9354021",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmlroots['0042']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf9b9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExtrating catalogue entries from xmls\")\n",
    "currentVolume = {k:v for k,v in sorted(xmlroots.items())}\n",
    "allLines = xmle.extractLinesForVol(list(currentVolume.values()))\n",
    "allLines = [line for line in allLines if line is not None]\n",
    "titles, allTitleIndices = xmle.findHeadings(allLines)\n",
    "titleRefs = xmle.genTitleRefs(allTitleIndices, allLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbcc117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveAll(currentVolume, directory, allTitleIndices, allLines, path, titleRefs):\n",
    "    # out_path = os.path.join(path, \"generated\")  # if you do want them to go into a sub_folder of the output loc\n",
    "    out_path = path\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "\n",
    "    xmle.savePoorlyScannedPages(xmle.getPoorlyScannedPages(currentVolume, directory), out_path)\n",
    "    print(\"Saving raw txt files\")\n",
    "    xmle.saveRawTxt(allTitleIndices, allLines, os.path.join(out_path, \"rawtextfiles\"), titleRefs)\n",
    "#     print(\"Saving split txt files\")\n",
    "#     xmle.saveSplitTxt(allTitleIndices, allLines, os.path.join(out_path, \"splittextfiles\"), titleRefs)\n",
    "    xmle.saveXML(allTitleIndices, allLines, out_path, titleRefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94fd447",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nSaving catalogue entries to {out_path}\\n\")\n",
    "saveAll(\n",
    "    currentVolume=list(currentVolume.values()),\n",
    "    directory=xmls,\n",
    "    allTitleIndices=allTitleIndices,\n",
    "    allLines=allLines,\n",
    "    path=out_path,\n",
    "    titleRefs=titleRefs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:incu]",
   "language": "python",
   "name": "conda-env-incu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
