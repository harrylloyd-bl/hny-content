{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5354fc4d",
   "metadata": {},
   "source": [
    "# Extracting zipped parsed page xml files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574f8a6f",
   "metadata": {},
   "source": [
    "This nb originally extracted the zip files containing the results of Isaac's parsing that were uploaded to the project [github](https://github.com/Southampton-Digital-Humanities/2023_Catalogue-Heading-Detection). Once we took over maintaining the code we've switched to processing the data locally so the zipping is obsolete. The majority of the code is now about parsing the xmls exported from Transkribus using updated versions of the code that Isaac wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cabb5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if '../' not in sys.path:\n",
    "    sys.path.append('../')\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "from xml.etree import ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "from src.data import xml_extraction as xmle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68affd28",
   "metadata": {},
   "source": [
    "### Extract zips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e1b9da",
   "metadata": {},
   "source": [
    "For when we were extracting zips of Isaac's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1e7834",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_data_path = \"\\\\\\\\ad\\\\collections\\\\TwoCenturies\\\\TwoCenturies IV\\\\Incunabula\\\\split_data\"\n",
    "\n",
    "zips = glob.glob(\"\\\\\\\\ad\\\\collections\\\\TwoCenturies\\\\TwoCenturies IV\\\\Incunabula\\\\split_data\\\\*.zip\")\n",
    "\n",
    "# [shutil.unpack_archive(x, x[:-4], format=\"zip\") for x in zips]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e6aa17",
   "metadata": {},
   "source": [
    "### Rename download jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e9c9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"..\\\\export_job_rename_dict.txt\") as f:  # .txt was a result of massaging the pairings out of Rossitza's .xlsx\n",
    "    rename_lines = f.readlines()\n",
    "    rename_split = [x.split(\":\") for x in rename_lines]\n",
    "    rename_dict = {x[0]:x[1].strip(\"\\n\") for x in rename_split}\n",
    "    \n",
    "rename_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb6bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for folder in done_zips:\n",
    "#     renamed = os.path.join(os.path.dirname(folder), export_dict[os.path.basename(folder)])\n",
    "#     os.rename(folder, renamed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76565f3d",
   "metadata": {},
   "source": [
    "#### All lines in entry honoured as separate lines in output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948dc109",
   "metadata": {},
   "source": [
    "This is a bit outdated now - I think Rossitza prefers to have all entries on one line for using in AntConc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dedef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_zip = done_zips[10:11]\n",
    "target_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0f128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in target_zip:\n",
    "    \n",
    "    txts = glob.glob(os.path.join(f, \"rawtextfiles\\\\*.txt\"))\n",
    "    raw_fname = os.path.join(f, \"combinedrawtext.txt\")\n",
    "    \n",
    "    with open(raw_fname, \"w\") as raw_out:\n",
    "        for t in tqdm(txts):\n",
    "            with open(t) as infile:\n",
    "                entry = infile.read()                \n",
    "                raw_out.write(entry)\n",
    "                raw_out.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d57268",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in target_zip:\n",
    "    print(f\"Combining  {f}\")\n",
    "    txts = glob.glob(os.path.join(f, \"splittextfiles\\\\*.txt\"))\n",
    "    split_fname = os.path.join(f, \"combinedsplittext.txt\")\n",
    "    \n",
    "    with open(split_fname, \"w\") as split_out:\n",
    "        for t in tqdm(txts):\n",
    "            with open(t) as infile:\n",
    "                entry = infile.readlines()\n",
    "                dash_line = \"-----------------------------------\\n\"\n",
    "                non_english_line = \"NON-ENGLISH SECTION LASTING\"\n",
    "                english_only_entry = [x for x in entry if dash_line not in x and non_english_line not in x]\n",
    "                split_str = \"\".join(english_only_entry)\n",
    "                split_out.write(split_str)\n",
    "                split_out.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb6f99b",
   "metadata": {},
   "source": [
    "#### All lines in entry concatenated onto one single line in output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbda4559",
   "metadata": {},
   "source": [
    "Better - but we're now combining all the entries from one volume together as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b30f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in target_zip:\n",
    "    \n",
    "    txts = glob.glob(os.path.join(f, \"rawtextfiles\\\\*.txt\"))\n",
    "    raw_fname = os.path.join(f, \"combinedrawtext_single_line.txt\")\n",
    "\n",
    "    with open(raw_fname, \"w\") as raw_out:\n",
    "        for t in tqdm(txts):\n",
    "            with open(t) as infile:\n",
    "                entry = infile.readlines()\n",
    "                raw_str = \" \".join([x.strip(\"\\n\") for x in entry])\n",
    "                raw_out.write(raw_str + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b598df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in target_zip:\n",
    "    print(f\"Combining  {f}\")\n",
    "    txts = glob.glob(os.path.join(f, \"splittextfiles\\\\*.txt\"))\n",
    "    split_fname = os.path.join(f, \"combinedsplittext_single_line.txt\")\n",
    "    \n",
    "    with open(split_fname, \"w\") as split_out:\n",
    "        for t in tqdm(txts):\n",
    "            with open(t) as infile:\n",
    "                entry = infile.readlines()\n",
    "                dash_line = \"-----------------------------------\\n\"\n",
    "                non_english_line = \"NON-ENGLISH SECTION LASTING\"\n",
    "                english_only_entry = [x.strip(\"\\n\") for x in entry if dash_line not in x and non_english_line not in x]\n",
    "                split_str = \" \".join(english_only_entry)\n",
    "                split_out.write(split_str + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c95fe",
   "metadata": {},
   "source": [
    "### Combined volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0e5936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paired globs\n",
    "combined_volumes = []\n",
    "for i in range(1):\n",
    "    network_loc = \"\\\\\\\\ad\\\\collections\\\\TwoCenturies\\\\TwoCenturies IV\\\\Incunabula\"\n",
    "    suffix = \"column pages Transkribus export\"\n",
    "    combined_volumes.append([os.path.join(network_loc, f\"BMC_{i+1} 2 {suffix}\"), os.path.join(network_loc, f\"BMC_{i+1} 4 {suffix}\")])    \n",
    "    \n",
    "#     When running locally\n",
    "#     combined_volumes.append([f'C:\\\\Users\\\\HLloyd\\\\Documents\\\\BMC_{i+1}_2', f'C:\\\\Users\\\\HLloyd\\\\Documents\\\\BMC_{i+1}_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a929a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079f41df",
   "metadata": {},
   "source": [
    "#### Raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d45d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(combined_volumes):\n",
    "    print(f\"Combining  {os.path.basename(v[0])}, {os.path.basename(v[1])}\")\n",
    "    txts = glob.glob(os.path.join(v[0], \"rawtextfiles\\\\*.txt\")) + glob.glob(os.path.join(v[1], \"rawtextfiles\\\\*.txt\"))\n",
    "    single_vol_dir = os.path.join(os.path.dirname(v[0]), \"split_data\\\\test\", f\"BMC_{i+1}\")\n",
    "    if not os.path.exists(single_vol_dir):\n",
    "        os.mkdir(single_vol_dir)\n",
    "    raw_fname = os.path.join(single_vol_dir, \"combinedrawtext_single_line.txt\")\n",
    "    \n",
    "#     with open(raw_fname, \"w\", encoding=\"utf-8-sig\") as raw_out:\n",
    "#         for t in tqdm(txts):\n",
    "#             with open(t, encoding=\"utf-8-sig\") as infile:\n",
    "#                 entry = infile.readlines()\n",
    "#                 raw_str = \" \".join([x.strip(\"\\n\") for x in entry])\n",
    "#                 raw_out.write(raw_str + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71999c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "txts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d0c37",
   "metadata": {},
   "source": [
    "#### Split text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4052078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(combined_volumes):\n",
    "    print(f\"Combining  {v[0]}\")\n",
    "    txts = glob.glob(os.path.join(v[0], \"splittextfiles\\\\*.txt\")) + glob.glob(os.path.join(v[1], \"splittextfiles\\\\*.txt\"))\n",
    "    single_vol_dir = os.path.join(os.path.dirname(v[0]), f\"BMC_{i+1}\")\n",
    "    if not os.path.exists(single_vol_dir):\n",
    "        os.mkdir(single_vol_dir)\n",
    "    split_fname = os.path.join(single_vol_dir, \"combinedsplittext_single_line.txt\")\n",
    "    \n",
    "    with open(split_fname, \"w\", encoding=\"utf-8-sig\") as split_out:\n",
    "        for t in tqdm(txts):\n",
    "            with open(t, encoding=\"utf-8-sig\") as infile:\n",
    "                entry = infile.readlines()\n",
    "                dash_line = \"-----------------------------------\\n\"\n",
    "                non_english_line = \"NON-ENGLISH SECTION LASTING\"\n",
    "                english_only_entry = [x.strip(\"\\n\") for x in entry if dash_line not in x and non_english_line not in x]\n",
    "                split_str = \" \".join(english_only_entry)\n",
    "                split_out.write(split_str + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66239890",
   "metadata": {},
   "source": [
    "### Refactor output check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e23c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "refactor_out = glob.glob(r\"C:\\Users\\HLloyd\\Documents\\BMC_*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e070607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "refactor_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b8286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in refactor_out:\n",
    "    shutil.copytree(x, os.path.join(ic_data_path, os.path.basename(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "refactor_sizes = {x: [os.path.getsize(y) for y in sorted(glob.glob(os.path.join(x, \"*.txt\")))] for x in refactor_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4715a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_out = glob.glob(r\"C:\\Users\\HLloyd\\Documents\\Incunabula-Catalogue-Entry-Detection\\data\\*\\rawtextfiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7deefe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acad605",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_paired_sizes = {x: [os.path.getsize(y) for y in sorted(glob.glob(os.path.join(x, \"*.txt\"))) if os.path.exists(os.path.join('C:\\\\Users\\\\HLloyd\\\\Documents\\\\', y[68:]))] for x in id_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a56e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor_sizes.pop(r\"C:\\Users\\HLloyd\\Documents\\BMC_5_2\\rawtextfiles\")\n",
    "for (me_k, me_v), (u_k, u_v) in zip(refactor_sizes.items(), id_paired_sizes.items()):\n",
    "    check = [x == y for x, y in zip(me_v, u_v)]\n",
    "    print(f\"{me_k[-25:-10]}: {sum(check) == len(check)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a53d11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "zips = glob.glob(r\"C:\\Users\\HLloyd\\Documents\\BMC*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437703ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf007522",
   "metadata": {},
   "outputs": [],
   "source": [
    "[shutil.unpack_archive(x, os.path.join(os.path.dirname(x), os.path.basename(x[:-4]))) for x in zips[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3ad586",
   "metadata": {},
   "source": [
    "## Running main.py to debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61250fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_xmls = []\n",
    "for i in range(1):\n",
    "    network_loc = \"../data/raw\"\n",
    "    suffix = \"column_model_output\"\n",
    "    page_xmls.append(\n",
    "        [\n",
    "            os.path.join(network_loc, f\"BMC_{i + 1}_2_{suffix}\"),\n",
    "            os.path.join(network_loc, f\"BMC_{i + 1}_4_{suffix}\")\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429d95e",
   "metadata": {},
   "source": [
    "Use 8 as the test bed as it's got one of the lowest numbers of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a4ae04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting xml roots from BMC_1  \n",
      "../data/raw\\BMC_1_2_column_model_output\\page \n",
      "../data/raw\\BMC_1_4_column_model_output\\page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 36.39it/s]\n"
     ]
    }
   ],
   "source": [
    "for x in page_xmls:\n",
    "    pageXMLLocation_2 = os.path.join(x[0], \"page/*.xml\")\n",
    "    pageXMLLocation_4 = os.path.join(x[1], \"page/*.xml\")\n",
    "    bmc_vol = \"_\".join(os.path.basename(x[0]).split(\"_\")[:2])\n",
    "    out_path = os.path.join(\"../data/processed\", bmc_vol)\n",
    "    attempts = 0\n",
    "    while attempts < 3:\n",
    "        xmls_2 = glob.glob(os.fsencode(pageXMLLocation_2))\n",
    "        xmls_4 = glob.glob(os.fsencode(pageXMLLocation_4))\n",
    "\n",
    "        if xmls_2 and xmls_4:\n",
    "            xmls = xmls_2 + xmls_4\n",
    "            break\n",
    "        else:\n",
    "            attempts += 1\n",
    "            continue\n",
    "    else:\n",
    "        raise IOError(f\"Failed to connect to {bmc_vol}  \\n{os.path.dirname(pageXMLLocation_2)} \\n{os.path.dirname(pageXMLLocation_4)}\")\n",
    "    \n",
    "    xmlroots = {}\n",
    "\n",
    "    print(f\"\\nGetting xml roots from {bmc_vol}  \\n{os.path.dirname(pageXMLLocation_2)} \\n{os.path.dirname(pageXMLLocation_4)}\")\n",
    "    for file in tqdm(xmls):\n",
    "        fileName = os.fsdecode(file)\n",
    "        attempts = 0\n",
    "        while attempts < 3:\n",
    "            try:\n",
    "                tree = ET.parse(fileName)\n",
    "                break\n",
    "            except FileNotFoundError:\n",
    "                attempts += 1\n",
    "                continue\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Failed to connect to: {fileName}\")\n",
    "        root = tree.getroot()\n",
    "        xmlroots[fileName.split(\"_\")[-1][:-4]] = root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cf9b9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extrating catalogue entries from xmls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 6661.86it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExtrating catalogue entries from xmls\")\n",
    "current_volume = {k: xmlroots[k] for k in sorted(xmlroots)}\n",
    "all_lines, xml_track_df = xmle.extract_lines_for_vol(current_volume)\n",
    "all_lines = [line for line in all_lines if line is not None]\n",
    "xml_track_df = xml_track_df.dropna(subset=\"line\")\n",
    "titles, all_title_indices = xmle.find_headings(all_lines)\n",
    "title_refs = xmle.gen_title_refs(all_title_indices, all_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bbcc117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 6666.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving raw txt files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 43/43 [00:00<00:00, 836.64it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)\n",
    "\n",
    "xmle.save_poorly_scanned_pages(xmle.get_poorly_scanned_pages(current_volume, xmls), out_path)\n",
    "print(\"Saving raw txt files\")\n",
    "xmle.save_raw_txt(all_title_indices, all_lines, xml_track_df, os.path.join(out_path, \"rawtextfiles\"), title_refs)\n",
    "# print(\"Saving split txt files\")\n",
    "# xmle.saveSplitTxt(all_title_indices, all_lines, os.path.join(out_path, \"splittextfiles\"), title_refs)\n",
    "xmle.save_xml(all_title_indices, all_lines, out_path, title_refs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:incu]",
   "language": "python",
   "name": "conda-env-incu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
